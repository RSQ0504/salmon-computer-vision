@inproceedings{10.1145/2764873.2764875,
author = {Wang, Nancy Xin Ru and Cullis-Suzuki, Sarika and Branzan Albu, Alexandra},
title = {Automated Analysis of Wild Fish Behavior in a Natural Habitat},
year = {2015},
isbn = {9781450335584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/2764873.2764875},
doi = {10.1145/2764873.2764875},
abstract = {This paper proposes a novel approach for the analysis of movement and behavior of the Plainfin midshipman (Porichthys notatus) in the wild. It is based on underwater video recordings of the fish in their natural habitat taken inside their nests during reproductive months. During this time, alpha male Plainfin midshipmen rarely leave their nests as they are guarding their eggs, so the proposed approach addresses the issue of detecting subtle motion and nesting behavior as the fish remains relatively sedentary. To the best of our knowledge, this is the first paper to propose an automated method to analyze subtle movements of a highly territorial animal in its natural habitat.Motion detection uses the displacement of SURF (Interest point algorithm) key-point movements from frame to frame to analyze the amount of movement by the fish. K-means clustering and other outlier removal techniques are then used to differentiate fish motion from small moving objects in the background and foreground. The analysis of fish behavior uses similarity-based periodicity detection combined with the K-neighbors classifier. Experimental validation with respect to expert-annotated ground truth shows excellent performance for both motion and behavior detection approaches.},
booktitle = {Proceedings of the 2nd International Workshop on Environmental Multimedia Retrieval},
pages = {21–26},
numpages = {6},
location = {Shanghai, China},
series = {EMR '15}
}

@inproceedings{10.1145/3325917.3325934,
author = {Manandhar, Nibha and Burris, John W.},
title = {An Application of Image Classification to Saltwater Fish Identification in Louisiana Fisheries},
year = {2019},
isbn = {9781450366359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/3325917.3325934},
doi = {10.1145/3325917.3325934},
abstract = {Fish identification is a challenge to recreational anglers, but critically important to the management of fisheries. The state of Louisiana currently provides printed illustrations of species as the sole aid to anglers for the process of fish identification. This work describes the application of Google's TensorFlow machine learning library to the task of fish identification as a case study on the application of the image classification capabilities. We describe the implementation and results of the project.},
booktitle = {Proceedings of the 2019 3rd International Conference on Information System and Data Mining},
pages = {129–132},
numpages = {4},
keywords = {Image classification, Machine learning, Fish identification},
location = {Houston, TX, USA},
series = {ICISDM 2019}
}

@inproceedings{10.1145/3055635.3056652,
author = {Kesvarakul, Ramil and Chianrabutra, Chamaporn and Chianrabutra, Srisit},
title = {Baby Shrimp Counting via Automated Image Processing},
year = {2017},
isbn = {9781450348171},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/3055635.3056652},
doi = {10.1145/3055635.3056652},
abstract = {The aim of this research is to investigate the method of detecting and counting baby shrimps by image processing technique. The experimental devices consist of a 1920x1080 pixels color image processing system and a light box. This light box is used as an indirect lighting source to avoid the bright spot from the target if the direct light technique is used. The shrimps are taken by a video camcorder, then the real time video is executed from the moving video to images and recorded them as the image files. The images are considered and converted to binary data types. Blob detection algorithm is used to detect the difference properties of color within the regions in the digital image. After that, the results from the image processing methodology are compared with the real value. It found that the shrimp quantity getting from the image processing technique is comparable and corresponding to the real shrimp quantity counting by manual with error is less than 7% of real quantity.},
booktitle = {Proceedings of the 9th International Conference on Machine Learning and Computing},
pages = {352–356},
numpages = {5},
keywords = {Image Processing, Counting, Baby Shrimp},
location = {Singapore, Singapore},
series = {ICMLC 2017}
}

@mastersthesis{Reithaug2018FishRec,
    author = {Reithaug, Adrian},
    institution = {The University of Bergen},
    school = {The University of Bergen},
    title = {Employing Deep Learning for Fish Recognition},
    year = 2018
}

@mastersthesis{Ghobrial2019FishSonar,
    author = {Ghobrial, Mina},
    institution = {University of Oulu},
    school = {University of Oulu},
    note = {\url{http://urn.fi/URN:NBN:fi:oulu-201906262667}},
    title = {Fish detection automation from ARIS and DIDSON SONAR data},
    year = 2019
}

@Article{Marini2018,
author={Marini, Simone
and Fanelli, Emanuela
and Sbragaglia, Valerio
and Azzurro, Ernesto
and Del Rio Fernandez, Joaquin
and Aguzzi, Jacopo},
title={Tracking Fish Abundance by Underwater Image Recognition},
journal={Scientific Reports},
year={2018},
month={Sep},
day={13},
volume={8},
number={1},
pages={13748},
abstract={Marine cabled video-observatories allow the non-destructive sampling of species at frequencies and durations that have never been attained before. Nevertheless, the lack of appropriate methods to automatically process video imagery limits this technology for the purposes of ecosystem monitoring. Automation is a prerequisite to deal with the huge quantities of video footage captured by cameras, which can then transform these devices into true autonomous sensors. In this study, we have developed a novel methodology that is based on genetic programming for content-based image analysis. Our aim was to capture the temporal dynamics of fish abundance. We processed more than 20,000 images that were acquired in a challenging real-world coastal scenario at the OBSEA-EMSO testing-site. The images were collected at 30-min. frequency, continuously for two years, over day and night. The highly variable environmental conditions allowed us to test the effectiveness of our approach under changing light radiation, water turbidity, background confusion, and bio-fouling growth on the camera housing. The automated recognition results were highly correlated with the manual counts and they were highly reliable when used to track fish variations at different hourly, daily, and monthly time scales. In addition, our methodology could be easily transferred to other cabled video-observatories.},
issn={2045-2322},
doi={10.1038/s41598-018-32089-8},
url={https://doi.org/10.1038/s41598-018-32089-8}
}

@article{DBLP:journals/corr/abs-1905-05241,
  author    = {Matias Valdenegro{-}Toro},
  title     = {Deep Neural Networks for Marine Debris Detection in Sonar Images},
  journal   = {CoRR},
  volume    = {abs/1905.05241},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.05241},
  archivePrefix = {arXiv},
  eprint    = {1905.05241},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-05241.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{10.1145/3340997.3340999,
author = {Yang, Xu and Gaspar, Jose and Lou, Weng Hong and Ke, Wei and Lam, Chan Tong and Wang, Yapeng},
title = {Vision-Based Mobile People Counting System},
year = {2019},
isbn = {9781450363235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/3340997.3340999},
doi = {10.1145/3340997.3340999},
abstract = {People detection and counting systems are highly valuable in multiple situations including managing emergency situations and efficiently allocating resources. However, most people counting systems are based on fixed sensors or fixed cameras, which lack flexibility and convenience. In this paper, we have developed a vision-based mobile people counting system which uses Android smartphones to capture images, and state-of-the-art person detectors, based on artificial intelligence, to count the number of people in a designated area. The embedded devices in smartphones such as camera, clock, GPS, are utilized to provide additional information for data collection. Several person detection frameworks such as You Only Look Once v2 (YOLO2), Aggregate Channel Features (ACF) and Multi-Task cascade Convolutional Neural Network (MTCNN) were evaluated to determine the best performing algorithm capable of offering accurate counting results across different scenarios. The experiments results show that YOLO2 outperforms ACF and MTCNN detection algorithms in different scenarios. However, YOLO2 has its own limitations as it often outputs redundant detections, requiring an additional Non-Maxima Suppression (NMS) algorithm to output a single bounding box per detection. The NMS threshold has to be carefully pre-fixed to provide top detection and counting performance across different scenarios.},
booktitle = {Proceedings of the 2019 4th International Conference on Machine Learning Technologies},
pages = {42–46},
numpages = {5},
keywords = {ACF, MTCNN, Mobile Application, People Counting System, YOLO2},
location = {Nanchang, China},
series = {ICMLT 2019}
}

@article{Zhang_2020,
    title={Automatic Fish Population Counting by Machine Vision and a Hybrid Deep Neural Network Model}, volume={10}, ISSN={2076-2615}, url={http://dx.doi.org/10.3390/ani10020364}, DOI={10.3390/ani10020364}, number={2}, journal={Animals}, publisher={MDPI AG}, author={Zhang, Song and Yang, Xinting and Wang, Yizhong and Zhao, Zhenxi and Liu, Jintao and Liu, Yang and Sun, Chuanheng and Zhou, Chao}, year={2020}, month={Feb}, pages={364}}

@article{10.1093/icesjms/fsx109,
    author = {Siddiqui, Shoaib Ahmed and Salman, Ahmad and Malik, Muhammad Imran and Shafait, Faisal and Mian, Ajmal and Shortis, Mark R and Harvey, Euan S},
    title = "{Automatic fish species classification in underwater videos: exploiting pre-trained deep neural network models to compensate for limited labelled data}",
    journal = {ICES Journal of Marine Science},
    volume = {75},
    number = {1},
    pages = {374-389},
    year = {2017},
    month = {07},
    abstract = "{There is a need for automatic systems that can reliably detect, track and classify fish and other marine species in underwater videos without human intervention. Conventional computer vision techniques do not perform well in underwater conditions where the background is complex and the shape and textural features of fish are subtle. Data-driven classification models like neural networks require a huge amount of labelled data, otherwise they tend to over-fit to the training data and fail on unseen test data which is not involved in training. We present a state-of-the-art computer vision method for fine-grained fish species classification based on deep learning techniques. A cross-layer pooling algorithm using a pre-trained Convolutional Neural Network as a generalized feature detector is proposed, thus avoiding the need for a large amount of training data. Classification on test data is performed by a SVM on the features computed through the proposed method, resulting in classification accuracy of 94.3\\% for fish species from typical underwater video imagery captured off the coast of Western Australia. This research advocates that the development of automated classification systems which can identify fish from underwater video imagery is feasible and a cost-effective alternative to manual identification by humans.}",
    issn = {1054-3139},
    doi = {10.1093/icesjms/fsx109},
    url = {https://doi.org/10.1093/icesjms/fsx109},
    eprint = {https://academic.oup.com/icesjms/article-pdf/75/1/374/31237461/fsx109.pdf},
}

@article{https://doi.org/10.1002/lom3.10113,
author = {Salman, Ahmad and Jalal, Ahsan and Shafait, Faisal and Mian, Ajmal and Shortis, Mark and Seager, James and Harvey, Euan},
title = {Fish species classification in unconstrained underwater environments based on deep learning},
journal = {Limnology and Oceanography: Methods},
volume = {14},
number = {9},
pages = {570-585},
doi = {https://doi.org/10.1002/lom3.10113},
url = {https://aslopubs.onlinelibrary.wiley.com/doi/abs/10.1002/lom3.10113},
eprint = {https://aslopubs.onlinelibrary.wiley.com/doi/pdf/10.1002/lom3.10113},
abstract = {Abstract Underwater video and digital still cameras are rapidly being adopted by marine scientists and managers as a tool for non-destructively quantifying and measuring the relative abundance, cover and size of marine fauna and flora. Imagery recorded of fish can be time consuming and costly to process and analyze manually. For this reason, there is great interest in automatic classification, counting, and measurement of fish. Unconstrained underwater scenes are highly variable due to changes in light intensity, changes in fish orientation due to movement, a variety of background habitats which sometimes also move, and most importantly similarity in shape and patterns among fish of different species. This poses a great challenge for image/video processing techniques to accurately differentiate between classes or species of fish to perform automatic classification. We present a machine learning approach, which is suitable for solving this challenge. We demonstrate the use of a convolution neural network model in a hierarchical feature combination setup to learn species-dependent visual features of fish that are unique, yet abstract and robust against environmental and intra-and inter-species variability. This approach avoids the need for explicitly extracting features from raw images of the fish using several fragmented image processing techniques. As a result, we achieve a single and generic trained architecture with favorable performance even for sample images of fish species that have not been used in training. Using the LifeCLEF14 and LifeCLEF15 benchmark fish datasets, we have demonstrated results with a correct classification rate of more than 90\%.},
year = {2016}
}

@inproceedings{10.1145/3357419.3357453,
author = {Hardjono, Benny and Rhizma, Mario G. A. and Widjaja, Andree E. and Tjahyadi, Hendra and Josodipuro, Madeleine Jose},
title = {Vehicle Counting Evaluation on Low-Resolution Images Using Software Tools},
year = {2019},
isbn = {9781450371889},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/3357419.3357453},
doi = {10.1145/3357419.3357453},
abstract = {Vehicle counting is an important parameter in building highway macroscopic model. This model ultimately will help highway designers, road planners and even common commuters, since it can give short term predictions of the road's behaviour which is influenced for example by its traffic flow, number of lanes, as well as off and on ramps. This research attempts to count vehicles from existing video cameras, which gives low-resolution 3 seconds video of 1 frame per second. For low-resolution, conventional methods, such as Back-subtraction, and Viola Jones are unable to give high counting accuracy. However, with the aid of another custom-made software tool, various parameters of Deep Learning method, such as pixel-frame distance thresholds, and two different counting models can be run repetitively, to obtain better accuracy. Early results have shown that by varying pixel distance threshold, the percentage of error can go down from 40.8% to as low as 0.8%.},
booktitle = {Proceedings of the 9th International Conference on Information Communication and Management},
pages = {89–94},
numpages = {6},
keywords = {Macroscopic model, Low-resolution image analysis, vehicle counting, deep learning, Virtual Detection Zone},
location = {Prague, Czech Republic},
series = {ICICM 2019}
}

@inproceedings{10.1145/3394171.3413945,
author = {Li, Wei and Wang, Zhenting and Wu, Xiao and Zhang, Ji and Peng, Qiang and Li, Hongliang},
title = {CODAN: Counting-Driven Attention Network for Vehicle Detection in Congested Scenes},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/3394171.3413945},
doi = {10.1145/3394171.3413945},
abstract = {Although recent object detectors have shown excellent performance for vehicle detection, they are incompetent for scenarios with a relatively large number of vehicles. In this paper, we explore the dense vehicle detection given the number of vehicles. Existing crowd counting methods cannot directly applied for dense vehicle detection due to insufficient description of density map, and the lack of effective constraint for mining the spatial awareness of dense vehicles. Inspired by these observations, a conceptually simple yet efficient framework, called CODAN, is proposed for dense vehicle detection. The proposed approach is composed of three major components: (i) an efficient strategy for generating multi-scale density maps (MDM) is designed to represent the vehicle counting, which can capture the global semantics and spatial information of dense vehicles, (ii) a multi-branch attention module (MAM) is proposed to bridging the gap between object counting and vehicle detection framework, (iii) with the well-designed density maps as explicit supervision, an effective counting-awareness loss (C-Loss) is employed to guide the attention learning by building the pixel-level constrain. Extensive experiments conducted on four benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art methods. The impressive results indicate that vehicle detection and counting can be mutually supportive, which is an important and meaningful finding.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {73–82},
numpages = {10},
keywords = {vehicle detection, deep learning, intelligent transportation system, vehicle counting},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{DBLP:journals/corr/RedmonF16,
  author    = {Joseph Redmon and
               Ali Farhadi},
  title     = {{YOLO9000:} Better, Faster, Stronger},
  journal   = {CoRR},
  volume    = {abs/1612.08242},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.08242},
  archivePrefix = {arXiv},
  eprint    = {1612.08242},
  timestamp = {Mon, 13 Aug 2018 16:48:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RedmonF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{6714453,
    author={P. {Dollár} and R. {Appel} and S. {Belongie} and P. {Perona}},  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},   title={Fast Feature Pyramids for Object Detection},   year={2014},  volume={36},  number={8},  pages={1532-1545},  doi={10.1109/TPAMI.2014.2300479}}

@INPROCEEDINGS{7299178,
    author={A. {Milan} and L. {Leal-Taixé} and K. {Schindler} and I. {Reid}},  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Joint tracking and segmentation of multiple targets},   year={2015},  volume={},  number={},  pages={5397-5406},  doi={10.1109/CVPR.2015.7299178}}

@inproceedings{10.1145/3265987.3265992,
author = {Lee, Hakjin and Ryu, Jongbin and Lim, Jongwoo},
title = {Joint Object Tracking and Segmentation with Independent Convolutional Neural Networks},
year = {2018},
isbn = {9781450359764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/3265987.3265992},
doi = {10.1145/3265987.3265992},
abstract = {Object tracking and segmentation are important research topics in computer vision. They provide the trajectory and boundary of an object based on their appearance and shape features. Most studies on tracking and segmentation focus on encoding methods for the feature of an object. However, the tracking trajectory and segmentation mask are acquired separately, although similar visual information is required for both methods. Therefore, in this paper, we propose a CNN-based joint object tracking and segmentation framework that provides a segmentation mask while improving the performance of object tacker. In our model, the tracking model determines the trajectory of the target object as a bounding box in each frame. Given the bounding box at each frame, the segmentation model predicts a dense mask of the target object in the bounding box. Then, the segmentation mask is used to refine the bounding box for the tracking model. We evaluate the performance of our algorithm on DAVIS benchmark dataset by AUC score and mean IoU. We showed that the performance of original tracker was improved by our proposed framework.},
booktitle = {Proceedings of the 1st Workshop and Challenge on Comprehensive Video Understanding in the Wild},
pages = {7–13},
numpages = {7},
keywords = {video object segmentation, object tracking, masking, joint model},
location = {Seoul, Republic of Korea},
series = {CoVieW'18}
}

@article{RAUF2019105075,
title = "Visual features based automated identification of fish species using deep convolutional neural networks",
journal = "Computers and Electronics in Agriculture",
volume = "167",
pages = "105075",
year = "2019",
issn = "0168-1699",
doi = "https://doi.org/10.1016/j.compag.2019.105075",
url = "http://www.sciencedirect.com/science/article/pii/S0168169919313523",
author = "Hafiz Tayyab Rauf and M. Ikram Ullah Lali and Saliha Zahoor and Syed Zakir Hussain Shah and Abd Ur Rehman and Syed Ahmad Chan Bukhari",
keywords = "Fish species classification, VGGNet, Deeply supervised VGGNet",
abstract = "Morphological based fish species identification is an erroneous and time-consuming process. There are numerous fish species and due to their close resemblance with each other, it is difficult to classify them by external characters. Recently, computer vision and deep learning-based identification of different animal species is being widely used by the researchers. Convolutional Neural Network (CNN) is one of the most analytically powerful tools in deep learning architecture for the image classification based on visual features. This work aims to propose a deep learning framework based on the CNN method for fish species identification. The proposed CNN architecture contains 32 deep layers that are considerably deep to derive valuable and discriminating features from the image. The deep supervision is inflicted on the VGGNet architecture to increase the classification performance by instantly adding four convolutional layers to the training of each level in the network. To test the performance of proposed 32-Layer CNN architecture, we developed a dataset termed as Fish-Pak and is publicly available at Mendeley data (Fish-Pak: https://doi.org/10.17632/n3ydw29sbz.3#folder-6b024354-bae3-460a-a758-352685ba0e38). Fish-Pak contains 915 images with six distinct classes; Ctenopharyngodon idella (Grass carp), Cyprinus carpio (Common carp), Cirrhinus mrigala (Mori), Labeo rohita (Rohu), Hypophthalmichthys molitrix (Silver carp), and Catla catla (Thala) and three different image views (head region, body shape, and scale). To ensure the superior performance of proposed CNN architecture, we have carried out the experimental comparison with other deep learning frameworks involving VGG-16 for transfer learning, one block VGG, two block VGG, three block VGG, LeNet-5, AlexNet, GoogleNet, and ResNet-50 on the Fish-Pak data set. Comprehensive empirical analyses reveal that the proposed method achieves state of the art performance and outperforms existing methods."
}

@inproceedings{10.1145/3361242.3361259,
author = {Lei, Cheng and Hu, Benlin and Wang, Dong and Zhang, Shu and Chen, Zhenyu},
title = {A Preliminary Study on Data Augmentation of Deep Learning for Image Classification},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.proxy.lib.sfu.ca/10.1145/3361242.3361259},
doi = {10.1145/3361242.3361259},
abstract = {Deep learning models have a large number of free parameters that need to be calculated by effective training of the models on a great deal of training data to improve their generalization performance. However, data obtaining and labeling is expensive in practice. Data augmentation is one of the methods to alleviate this problem. In this paper, we conduct a preliminary study on how four variables (augmentation method, augmentation rate, size of basic dataset per label, and method combination) can affect the accuracy of deep learning for image classification. The study provides some guidelines: (1) altering the geometry of the images is not always better than those just lighting and color. (2) 2-3 times augmentation rate is good enough for training. (3) the combination of two geometry methods degrade the performance, while combinations with at least one photometric method, will improve the performance, especially when one method is a photometric method and another is a geometry method. (4) the sequence of methods in combination has little effect on the performance.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {20},
numpages = {6},
keywords = {data quality, quality assurance, Data augmentation, model quality},
location = {Fukuoka, Japan},
series = {Internetware '19}
}
